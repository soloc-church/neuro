{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fea066",
   "metadata": {},
   "source": [
    "Seek his will in all you do, and he will show you which path to take.\n",
    "Proverbs 3:6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6bc99",
   "metadata": {},
   "source": [
    "Use QAOA to verify the partitioning of neurons by cortical depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a061f0",
   "metadata": {},
   "source": [
    "\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979ca13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: A required dependency is missing. No module named 'numba'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'njit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResponseProcessor, NetworkAnalyzer, DimensionalityReducer\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_session_comprehensive, analyze_session_quality\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m features\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/pack/qbraid/neuroscience-qaoa/src/features.py:392\u001b[39m\n\u001b[32m    385\u001b[39m             isi_rows.append({\n\u001b[32m    386\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33munit\u001b[39m\u001b[33m'\u001b[39m: unit_id, \n\u001b[32m    387\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mmin_isi_s\u001b[39m\u001b[33m'\u001b[39m: np.min(isi_s), \n\u001b[32m    388\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mmedian_isi_s\u001b[39m\u001b[33m'\u001b[39m: np.median(isi_s)\n\u001b[32m    389\u001b[39m             })\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(isi_rows)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;129m@njit\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_find_triplet_instances_fast\u001b[39m(t1: np.ndarray, t2: np.ndarray, t3: np.ndarray, max_gap: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m    394\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Numba-accelerated function to find raw time tuples for a pattern.\"\"\"\u001b[39;00m\n\u001b[32m    395\u001b[39m     out_times = []\n",
      "\u001b[31mNameError\u001b[39m: name 'njit' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from scipy import stats, signal\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.stats import binomtest, binned_statistic \n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mutual_info_score  \n",
    "from sklearn.metrics import silhouette_score\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from pathlib import Path\n",
    "import sys\n",
    "PROJECT_ROOT = Path().resolve().parent  \n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.dataio.config import DATA_DIR, FILESYSTEM_CONFIG\n",
    "from src.dataio.data_structures import SessionData\n",
    "from src.dataio.loaders import load_session_complete\n",
    "from src.dataio.processors import ResponseProcessor, NetworkAnalyzer, DimensionalityReducer\n",
    "from src.dataio.validators import validate_session_comprehensive, analyze_session_quality\n",
    "from src import features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc8df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 9 sessions:\n",
      "   031020_367n_100um20st_FRA\n",
      "   031020_367r_100um20st_FRA\n",
      "   031120_352ll_100um20st_FRA\n",
      "   031120_352ll_100um20st_FRA_diffxy\n",
      "   080720_400r\n",
      "   081820_355n\n",
      "   081920_355l\n",
      "   081920_355r\n",
      "   082620_355l\n",
      "\n",
      "Primary analysis session: 081820_355n\n",
      "Loading session from: ../data/raw/081820_355n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No signal correlation matrix was found in any source.\n",
      "No noise correlation matrix was found in any source.\n",
      "Trimmed trials to 22 to match available data\n",
      "VALIDATION WARNING: Unexpected trial count: 22. Expected one of [90, 180]\n",
      "VALIDATION WARNING: Activity matrix contains 4 NaN values (0.00%).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Session loaded successfully!\n",
      "Neurons: 416\n",
      "Trials:  22\n",
      "Activity matrix shape: (416, 2060)\n",
      "Data quality score: 1.00\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path('../data/raw')\n",
    "\n",
    "# Find all subdirs that contain the expected .mat file\n",
    "MAT_FILENAME = 'allPlanesVariables27-Feb-2021.mat'\n",
    "sessions = sorted([\n",
    "    d.name\n",
    "    for d in DATA_DIR.iterdir()\n",
    "    if d.is_dir() and (d / MAT_FILENAME).is_file()\n",
    "])\n",
    "\n",
    "print(f\"Discovered {len(sessions)} sessions:\")\n",
    "for s in sessions:\n",
    "    print(\"  \", s)\n",
    "\n",
    "# Choose your primary session here (must be one of the discovered names)\n",
    "PRIMARY_SESSION = '081820_355n'\n",
    "if PRIMARY_SESSION not in sessions:\n",
    "    raise ValueError(f\"Primary session '{PRIMARY_SESSION}' not found in data/. Available: {sessions}\")\n",
    "\n",
    "print(f\"\\nPrimary analysis session: {PRIMARY_SESSION}\")\n",
    "\n",
    "# --- now load & process as before ---\n",
    "\n",
    "session_path = DATA_DIR / PRIMARY_SESSION\n",
    "print(f\"Loading session from: {session_path}\")\n",
    "\n",
    "session = load_session_complete(session_path, use_cache=True)\n",
    "\n",
    "validation_result = validate_session_comprehensive(session)\n",
    "quality_report     = analyze_session_quality(session)\n",
    "\n",
    "print(f\"\\nSession loaded successfully!\")\n",
    "print(f\"Neurons: {len(session.neurons)}\")\n",
    "print(f\"Trials:  {len(session.trials)}\")\n",
    "print(f\"Activity matrix shape: {session.activity_matrix.shape}\")\n",
    "print(f\"Data quality score: {quality_report['overall_score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b17eb7",
   "metadata": {},
   "source": [
    "## Natural Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ed0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Part 1: Natural Assembly Discovery\n",
    "# ===================================================================\n",
    "# We begin by identifying neuronal assemblies without making assumptions about\n",
    "# their number. We use hierarchical clustering based on activity correlations.\n",
    "# The key step is to find the optimal number of clusters.\n",
    "\n",
    "print(\"--- Part 1: Discovering Natural Assemblies ---\")\n",
    "\n",
    "# --- 1.1: Calculate Correlation and Linkage Matrices ---\n",
    "# Use the new, clean functions from src/features.py\n",
    "if hasattr(session, 'signal_correlations') and session.signal_correlations is not None:\n",
    "    print(\"Using pre-computed signal correlations.\")\n",
    "    corr_matrix = session.signal_correlations\n",
    "else:\n",
    "    print(\"Computing correlation matrix from activity data...\")\n",
    "    corr_matrix = features.pearson_corr_matrix(session.activity_matrix)\n",
    "\n",
    "full_corr = corr_matrix.copy() # Keep a copy for later use\n",
    "linkage_matrix = features.calculate_linkage_matrix(corr_matrix, method='ward')\n",
    "\n",
    "# --- 1.2: Determine Optimal Number of Clusters ---\n",
    "print(\"Determining optimal number of clusters using silhouette score...\")\n",
    "n_clusters_range = range(5, 51)\n",
    "optimal_n_clusters, silhouette_scores = features.find_optimal_clusters_silhouette(\n",
    "    linkage_matrix, corr_matrix, n_clusters_range\n",
    ")\n",
    "cluster_labels = features.get_cluster_labels(linkage_matrix, optimal_n_clusters)\n",
    "unique_clusters = np.unique(cluster_labels)\n",
    "\n",
    "print(f\"Found optimal number of clusters: {optimal_n_clusters}\")\n",
    "\n",
    "# --- 1.3: Visualization and Statistics ---\n",
    "# Visualization logic remains in the notebook.\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot Silhouette Score to justify our choice\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(n_clusters_range, silhouette_scores, 'o-')\n",
    "plt.axvline(optimal_n_clusters, color='r', linestyle='--', label=f'Optimal n = {optimal_n_clusters}')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Dendrogram with the determined cluster threshold\n",
    "plt.subplot(2, 2, 2)\n",
    "cut_distance = linkage_matrix[-(optimal_n_clusters - 1), 2]\n",
    "hierarchy.dendrogram(linkage_matrix, no_labels=True, color_threshold=cut_distance)\n",
    "plt.axhline(cut_distance, color='r', linestyle='--', label=f'Cut for {optimal_n_clusters} clusters')\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Neuron (grouped)')\n",
    "plt.ylabel('Distance (1 - |Correlation|)')\n",
    "plt.legend()\n",
    "\n",
    "# Reorder correlation matrix by the found clusters\n",
    "cluster_order = np.argsort(cluster_labels)\n",
    "corr_matrix_clustered = corr_matrix[cluster_order][:, cluster_order]\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(corr_matrix_clustered, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.title(f'Correlation Matrix (Clustered, n={optimal_n_clusters})')\n",
    "plt.xlabel('Neuron Index (sorted by cluster)')\n",
    "plt.ylabel('Neuron Index (sorted by cluster)')\n",
    "cluster_boundaries = np.where(np.diff(cluster_labels[cluster_order]))[0] + 0.5\n",
    "for boundary in cluster_boundaries:\n",
    "    plt.axhline(boundary, color='black', lw=0.5)\n",
    "    plt.axvline(boundary, color='black', lw=0.5)\n",
    "\n",
    "# Cluster size distribution\n",
    "plt.subplot(2, 2, 4)\n",
    "cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "plt.bar(cluster_sizes.index, cluster_sizes.values)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Number of neurons')\n",
    "plt.title('Cluster Size Distribution')\n",
    "plt.xlim([0, optimal_n_clusters + 1])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.suptitle('Natural Assembly Discovery and Validation', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# --- 1.4: Cluster Co-activation ---\n",
    "cluster_activity_matrix, cluster_corr = features.calculate_cluster_coactivation(\n",
    "    session.activity_matrix, cluster_labels\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cluster_corr, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation of Mean Cluster Activity')\n",
    "plt.title('Cluster Co-activation Matrix')\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Cluster ID')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Natural Assembly Statistics ---\")\n",
    "print(f\"Number of natural clusters found: {optimal_n_clusters}\")\n",
    "print(f\"Cluster sizes: Min={cluster_sizes.min()}, Max={cluster_sizes.max()}, Mean={cluster_sizes.mean():.1f} ± {cluster_sizes.std():.1f}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Part 2: Spatial and Functional Analysis of Assemblies\n",
    "# ===================================================================\n",
    "print(\"\\n--- Part 2: Spatial and Functional Analysis ---\")\n",
    "# Data preparation for this section\n",
    "neuron_coords = np.array([[n.x, n.y, n.z] for n in session.neurons])\n",
    "neuron_planes = np.array([n.plane for n in session.neurons])\n",
    "bf_values = np.array([getattr(n, 'best_frequency', np.nan) for n in session.neurons], dtype=float)\n",
    "\n",
    "# --- 2.1: Spatial Coherence Analysis ---\n",
    "print(\"Analyzing spatial coherence with permutation testing...\")\n",
    "cluster_spatial_stats = features.analyze_spatial_coherence(\n",
    "    neuron_coords, cluster_labels, n_permutations=1000\n",
    ")\n",
    "\n",
    "# --- 2.2: Functional (Best Frequency) Coherence Analysis ---\n",
    "print(\"Analyzing functional (Best Frequency) coherence...\")\n",
    "cluster_bf_stats = features.analyze_functional_coherence(\n",
    "    bf_values, cluster_labels, n_permutations=1000\n",
    ")\n",
    "\n",
    "# --- 2.3: Visualization of Spatial and Functional Properties ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14), gridspec_kw={'height_ratios': [1, 1]})\n",
    "# a) Spatial distribution in XY plane\n",
    "ax = axes[0, 0]\n",
    "scatter = ax.scatter(neuron_coords[:, 0], neuron_coords[:, 1], s=10, c=cluster_labels, cmap='tab20', alpha=0.8)\n",
    "ax.set_xlabel('X coordinate (µm)'); ax.set_ylabel('Y coordinate (µm)')\n",
    "ax.set_title('Spatial Map of Neurons Colored by Cluster')\n",
    "ax.legend(handles=scatter.legend_elements(num=min(20, optimal_n_clusters))[0],\n",
    "          labels=[f'C{i}' for i in range(1, min(20, optimal_n_clusters) + 1)],\n",
    "          bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# b) Spatial Coherence Results\n",
    "ax = axes[0, 1]\n",
    "compact_clusters = {k: v for k, v in cluster_spatial_stats.items() if v.get('is_compact')}\n",
    "distrib_clusters = {k: v for k, v in cluster_spatial_stats.items() if not v.get('is_compact')}\n",
    "if compact_clusters:\n",
    "    ax.bar(compact_clusters.keys(), [v['mean_dist'] for v in compact_clusters.values()], color='blue', label='Spatially Compact (p<0.05)')\n",
    "if distrib_clusters:\n",
    "    ax.bar(distrib_clusters.keys(), [v['mean_dist'] for v in distrib_clusters.values()], color='gray', label='Spatially Distributed (p>=0.05)')\n",
    "ax.set_xlabel('Cluster ID'); ax.set_ylabel('Mean Intra-cluster Distance (µm)')\n",
    "ax.set_title('Spatial Coherence of Clusters (vs. Random)'); ax.legend()\n",
    "# c) Distribution across planes\n",
    "ax = axes[1, 0]\n",
    "plane_cluster_matrix = pd.crosstab(neuron_planes, cluster_labels)\n",
    "im = ax.imshow(plane_cluster_matrix, aspect='auto', cmap='viridis', interpolation='none')\n",
    "ax.set_xlabel('Cluster ID'); ax.set_ylabel('Imaging Plane')\n",
    "ax.set_yticks(ticks=range(len(plane_cluster_matrix.index)), labels=plane_cluster_matrix.index)\n",
    "ax.set_title('Cluster Distribution Across Imaging Planes'); plt.colorbar(im, ax=ax, label='Neuron Count')\n",
    "# d) Functional (BF) Coherence Results\n",
    "ax = axes[1, 1]\n",
    "homog_clusters = {k: v for k, v in cluster_bf_stats.items() if v.get('is_homog')}\n",
    "hetero_clusters = {k: v for k, v in cluster_bf_stats.items() if not v.get('is_homog')}\n",
    "if homog_clusters:\n",
    "    ax.errorbar(homog_clusters.keys(), [v['mean_bf'] for v in homog_clusters.values()], yerr=[10**v['std_log_bf'] for v in homog_clusters.values()], fmt='o', color='green', label='Homogeneous BF (p<0.05)')\n",
    "if hetero_clusters:\n",
    "    ax.errorbar(hetero_clusters.keys(), [v['mean_bf'] for v in hetero_clusters.values()], yerr=[10**v['std_log_bf'] for v in hetero_clusters.values()], fmt='o', color='gray', label='Heterogeneous BF (p>=0.05)')\n",
    "ax.set_yscale('log'); ax.set_xlabel('Cluster ID'); ax.set_ylabel('Best Frequency (Hz)')\n",
    "ax.set_title('Functional Coherence of Clusters (vs. Random)'); ax.legend()\n",
    "ax.grid(True, which=\"both\", ls=\"--\", axis='y')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.suptitle('Spatial and Functional Properties of Assemblies', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# --- Summary Printout ---\n",
    "print(\"\\n--- Spatial and Functional Summary ---\")\n",
    "if cluster_spatial_stats:\n",
    "    n_compact = sum(v.get('is_compact', False) for v in cluster_spatial_stats.values())\n",
    "    print(f\"{n_compact} out of {len(cluster_spatial_stats)} clusters are significantly compact.\")\n",
    "if cluster_bf_stats:\n",
    "    n_homog = sum(v.get('is_homog', False) for v in cluster_bf_stats.values())\n",
    "    print(f\"{n_homog} out of {len(cluster_bf_stats)} clusters have significantly homogeneous BFs.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Part 3: Stimulus-Specificity of Assemblies\n",
    "# ===================================================================\n",
    "print(\"\\n--- Part 3: Stimulus Specificity Analysis ---\")\n",
    "if session.trials:\n",
    "    print(\"Found trial data. Analyzing cluster responses...\")\n",
    "    \n",
    "    # Prepare stimulus table from session data\n",
    "    stim_table = pd.DataFrame([t.to_dict() for t in session.trials])\n",
    "    stim_table.rename(columns={'condition': 'stimulus_name'}, inplace=True)\n",
    "    \n",
    "    # Calculate cluster response matrix using the new feature function\n",
    "    cluster_response_matrix = features.calculate_stimulus_response_matrix(\n",
    "        cluster_activity_matrix, unique_clusters, stim_table, session.activity_matrix.shape\n",
    "    )\n",
    "\n",
    "    # Normalize responses for visualization (z-score across stimuli for each cluster)\n",
    "    cluster_response_zscored = cluster_response_matrix.apply(\n",
    "        lambda x: zscore(x, nan_policy='omit') if x.std(ddof=0) > 0 else x,\n",
    "        axis=1, result_type='expand'\n",
    "    )\n",
    "    cluster_response_zscored.columns = cluster_response_matrix.columns\n",
    "    \n",
    "    # --- Visualization ---\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    im = plt.imshow(cluster_response_zscored.fillna(0), cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar(label='Z-scored Mean Activity during Stimulus')\n",
    "    plt.title('Cluster Selectivity to Stimuli')\n",
    "    plt.xlabel('Stimulus Name'); plt.ylabel('Cluster ID')\n",
    "    plt.xticks(ticks=range(len(cluster_response_matrix.columns)), labels=cluster_response_matrix.columns, rotation=90)\n",
    "    plt.yticks(ticks=range(len(cluster_response_matrix.index)), labels=cluster_response_matrix.index)\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Identify Core vs. Specific Clusters ---\n",
    "    max_zscores = cluster_response_zscored.max(axis=1)\n",
    "    specific_clusters = max_zscores[max_zscores > 2.0].index.tolist()\n",
    "    core_clusters = max_zscores.index.difference(specific_clusters).tolist()\n",
    "\n",
    "    print(\"\\n--- Stimulus Specificity Summary ---\")\n",
    "    print(f\"Found {len(specific_clusters)} potentially stimulus-specific clusters (max response > 2 Z).\")\n",
    "    print(f\"IDs: {specific_clusters}\")\n",
    "    print(f\"\\nFound {len(core_clusters)} potential core/non-selective clusters.\")\n",
    "    print(f\"IDs: {core_clusters}\")\n",
    "else:\n",
    "    print(\"Skipping stimulus-specificity analysis: 'session.trials' not found or is empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
